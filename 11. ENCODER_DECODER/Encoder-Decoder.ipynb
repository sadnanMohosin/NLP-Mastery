{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Encoder-Decoder?\n",
    "The Encoder-Decoder model is mainly a concept in the NLP field. It does not specifically value a specific algorithm, but a general term for a class of algorithms. Encoder-Decoder can be regarded as a general framework, under which different algorithms can be used to solve different tasks.\n",
    "\n",
    "The Encoder-Decoder framework is a good illustration of the core ideas of machine learning:\n",
    "\n",
    "Encoder is also called an encoder. Its role is to \"transform real problems into mathematical problems\"\n",
    "\n",
    "<img src=\"img/en.png\">\n",
    "\n",
    "\n",
    "Decoder, also known as decoder, is used to \"solve mathematical problems and translate them into real-world solutions\"\n",
    "\n",
    "\n",
    "\n",
    "The two links are connected, and the general diagram is as follows:\n",
    "\n",
    "\n",
    "<img src=\"img/en1.jpeg\">\n",
    "\n",
    "Regarding Encoder-Decoder, there are 2 points to be explained:\n",
    "\n",
    "Regardless of the length of the input and output, the length of the \"vector c\" in the middle is fixed (this is also its defect, which will be explained in detail below)\n",
    "\n",
    "You can choose different encoders and decoders according to different tasks (it can be an RNN , but it is usually a variant of LSTM or GRU )\n",
    "\n",
    "As long as it conforms to the above framework, it can be collectively referred to as the Encoder-Decoder model. When it comes to the Encoder-Decoder model, a term is often mentioned-Seq2Seq.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder model and implementation of RNN\n",
    "\n",
    "Encoder-Decoder (encoding-decoding) is a very common model framework in deep learning. For example, auto-encoding of unsupervised algorithms is designed and trained with the structure of encoding-decoding; Is the encoding-decoding framework of CNN-RNN; for example, the neural network machine translation NMT model is often the LSTM-LSTM encoding-decoding framework. \n",
    "\n",
    ">Therefore, to be precise, Encoder-Decoder is not a specific model, but a kind of framework. Encoder and Decoder parts can be any text, voice, image, video data, and models can use CNN, RNN, BiRNN, LSTM, GRU, etc. So based on Encoder-Decoder, we can design a variety of application algorithms.\n",
    "\n",
    "One of the most significant features of the Encoder-Decoder framework is that it is an End-to-End learning algorithm. Such models are often used in machine translation, such as translating French to English . Such a model is also called Sequence to Sequence learning. The so-called encoding is to convert the input sequence into a fixed-length vector and decoding is to convert the previously generated fixed vector into an output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder-Decoder framework intuition\n",
    "\n",
    "It can be considered as a general processing model suitable for processing one sentence (or chapter) to generate another sentence (or chapter). For sentence pair <X, Y>, our goal is to give the input sentence X, and expect to generate the target sentence Y through the Encoder-Decoder framework. X and Y can be the same language or two different languages. X and Y are composed of their respective word sequences:\n",
    "\n",
    "<img src=\"img/en6.jpg\">\n",
    "\n",
    "Encoder, as its name implies, encodes the input sentence X, and transforms the input sentence into an intermediate semantic representation C through a non-linear transformation.\n",
    "<img src=\"img/en7.jpg\">\n",
    "\n",
    "For the decoder Decoder, its task is based on the sentence X The middle semantics means that C and the historical information y1, y2 .... yi-1 that have been generated before to generate the word yi to be generated at time i.\n",
    "\n",
    "<img src=\"img/en8.jpg\">\n",
    "\n",
    ">**Encoder-Decoder is not a model, but a framework, a way to deal with problems.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "Encoder-Decoder is a model framework in the field of NLP. It is widely used for tasks such as machine translation and speech recognition.\n",
    "\n",
    ">Machine translation, dialogue robot, poetry generation, code completion, article abstract (text-text)\n",
    "\n",
    "\"Text-text\" is the most typical application, and the length of the input sequence and output sequence may be quite different.\n",
    "\n",
    "Google's paper \" <a href=\"https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\" target=\"_blank\">Sequence to Sequence Learning with Neural Networks</a> \" using Seq2Seq for machine translation.\n",
    "\n",
    "<img src=\"img/en3.webp\">\n",
    "\n",
    "\n",
    "### Speech recognition (audio-text)\n",
    "\n",
    "Speech recognition also has strong sequence features, which is more suitable for the Encoder-Decoder model.\n",
    "\n",
    "Google's paper \" <a href=\"https://research.google/pubs/pub46169/\" target=\"_blank\">A Comparison of Sequence-to-Sequence Models for Speech Recognition</a> \" using Seq2Seq for speech recognition\n",
    "\n",
    "<img src=\"img/en4.webp\">\n",
    "\n",
    "\n",
    "### Image description generation (Picture-Text)\n",
    "\n",
    "Popularly speaking is \"seeing pictures and speaking\", the machine extracts the features of the pictures, and then expresses them in words. This application is a combination of computer vision and NLP.\n",
    "\n",
    "Image description generated paper \" <a href=\"https://arxiv.org/abs/1505.00487\" target=\"_blank\">Sequence to Sequence â€“ Video to Text</a> \"\n",
    "\n",
    "<img src=\"img/en5.webp\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Encoder-Decoder flaws\n",
    "As mentioned above, there is only one \"vector c\" between the Encoder and the Decoder, and the length of c is fixed.\n",
    "\n",
    "For the sake of understanding, we analogize the process of \"compression-decompression\":\n",
    "\n",
    "Compressing an 800x800 pixel image into 100KB, it looks relatively clear. Compress another 3000X3000 pixel image to 100KB and it looks blurry.\n",
    "\n",
    "<img src=\"img/test.jpg\">\n",
    "\n",
    ">**Encoder-Decoder is a similar problem: when the input information is too long, some information will be lost.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the encoder-decoder model is very classic, its limitations are also very large. \n",
    "\n",
    "The biggest limitation is that the only connection between encoding and decoding is a fixed-length semantic vector C. That is, the encoder compresses the entire sequence of information into a fixed-length vector. \n",
    "\n",
    "However, there are two disadvantages to this. \n",
    "\n",
    "1. **One is that the semantic vector cannot completely represent the information of the entire sequence.**\n",
    "\n",
    "\n",
    "2. **The other is that the information carried by the content entered first will be diluted by the information entered later, or covered. The longer the input sequence, the more severe this phenomenon is.**\n",
    "\n",
    "\n",
    "\n",
    ">**This makes it impossible to obtain enough information about the input sequence at the beginning of decoding, so the accuracy of decoding will naturally be compromised.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
